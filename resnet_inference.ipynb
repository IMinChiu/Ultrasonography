{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNeSmlplpfarDzWImuqly/B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/outofray/Ultrasonography/blob/main/resnet_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q__ZOHjMooCz"
      },
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwAlu9gOpGOA",
        "outputId": "9102701c-0414-4ed8-b82a-2d1ca973a414"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ue45Ld7pduo"
      },
      "source": [
        "input_shape = (400, 400, 3)\n",
        "filename = \"gdrive/MyDrive/morrison pouch real time feedback/demo 3.avi\"\n",
        "class_names = [\"Ascites: Negative.     Quality: High.\", \"Ascites: Negative.     Quality: Low.\", \"Ascites: Positive.     Quality: High.\", \"Ascites: Positive.     Quality: Low.\"]\n",
        "weight_file = \"gdrive/MyDrive/morrison pouch real time feedback/morrison_4class_weights_ResNet50V2_96.hdf5\"\n",
        "data_augmentation = tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_VLyQq4_YMy",
        "outputId": "961674a8-7555-4ea9-c7b0-4428f6bdb579"
      },
      "source": [
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ascites: Negative.     Quality: High.', 'Ascites: Negative.     Quality: Low.', 'Ascites: Positive.     Quality: High.', 'Ascites: Positive.     Quality: Low.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb0n7jcpphJo"
      },
      "source": [
        "def ResNet50V2 (img_array):\n",
        "\n",
        "    base_model = tf.keras.applications.resnet_v2.ResNet50V2(\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\", #pre-trained from imagenet\n",
        "        input_shape=input_shape)\n",
        "\n",
        "    base_model.trainable = True  #set weights to be trainable\n",
        "\n",
        "    # Fine-tune from this layer onwards\n",
        "    fine_tune_at = 96\n",
        "\n",
        "    # Freeze all the layers before the `fine_tune_at` layer\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # add top layer\n",
        "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    batchnorm = tf.keras.layers.BatchNormalization()\n",
        "    prediction_layer = tf.keras.layers.Dense(4)\n",
        "\n",
        "    # put model together\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = base_model(x, training=False)\n",
        "    x = global_average_layer(x)\n",
        "    x = batchnorm(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    outputs = prediction_layer(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    base_learning_rate = 0.0001\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.load_weights(weight_file)\n",
        "    predictions = model.predict(img_array)\n",
        "    score = tf.nn.softmax(predictions[0])\n",
        "    return (score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r553IraG7T_Y"
      },
      "source": [
        "# open video\n",
        "cap = cv2.VideoCapture(filename)\n",
        "\n",
        "res=(400,400) #resulotion\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID') #codec\n",
        "out = cv2.VideoWriter('gdrive/MyDrive/morrison pouch real time feedback/inference.mp4', fourcc, 12.0, res)\n",
        "\n",
        "# open frames in loop\n",
        "while(cap.isOpened()):\n",
        "  ret, frame = cap.read()\n",
        "\n",
        "  # set and crop zone of interest\n",
        "  x = 140\n",
        "  y = 40\n",
        "  w = 400\n",
        "  h = 400\n",
        "  frame_cropped = frame[y:y + h, x:x + w]\n",
        "\n",
        "  # process frames\n",
        "  img_array = tf.keras.preprocessing.image.img_to_array(frame_cropped)\n",
        "  img_array = tf.expand_dims(img_array, 0)  # Create a batch\n",
        "\n",
        "  # get model prediction score\n",
        "  score = ResNet50V2(img_array)\n",
        "\n",
        "  result = \"{} ({:.2f}%)\".format(class_names[np.argmax(score)], 100 * np.max(score))\n",
        "\n",
        "  feedback = \"<<Hold the probe>>\"\n",
        "\n",
        "  # feedback prediction on frame\n",
        "  cv2.putText(frame_cropped, result, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "  if np.argmax(score) in [0, 2]:\n",
        "    cv2.putText(frame_cropped, feedback, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "  cv2_imshow(frame_cropped)\n",
        "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "    break\n",
        "\n",
        "  out.write(frame_cropped)\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}